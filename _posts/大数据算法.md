# 停机问题

不失一般性, 假设我们想要测试的代码是一个函数

```cpp
void f(char *t);
```

其中t是一个字符串, 我们可以用一个字符串表示任意输入, 包括`int`, `double`, 及复合数据类型; 当f不需要输入时, `t`是一个空字符串. 停机问题是指对给定任意的一个函数`f`和输入`t`, 判断`f`对`t`是否会永远运行下去; 如果程序的运行时间是有限长的, 我们称`f`对`t`**停机**.



# seidel's LP algo

我们现在描述一种由Raimund Seidel提出的线性编程算法，该算法能在O(m)时间内解决2维（即2变量）LP问题（回顾一下，m是约束条件的数量），更普遍的是在O(d!m)时间内解决d维LP问题。

设置。我们有d个变量x1, . . . , xd。我们得到了这些变量的m个线性约束a1 - x ≤ b1, . . . ，am - x ≤ bm，同时还有一个目标c - x要最大化。(用黑体字表示向量。)我们的目标是找到一个满足约束条件的解决方案x，使目标最大化。

思路：这里是塞德尔算法的思路。让我们一次一次地加入约束条件，并跟踪到目前为止的约束条件的最优解。例如，假设我们已经找到了
前m-1个约束条件的最优解x∗（现在我们假设到目前为止的约束条件不允许c-x的值无限大），现在我们加入第m个约束条件am -x ≤ bm。
有两种情况需要考虑。

情况1：如果x∗满足约束条件，那么x∗仍然是最优的。执行这个测试的时间。O(d)。
情况2：如果x∗不满足约束条件，那么新的最优点将在（d - 1）-维超平面am - x = bm上，否则就没有可行的点。
现在让我们关注d=2的情况，并考虑处理上述情况2的时间。在d = 2的情况下，超平面am - x = bm只是一条线，让我们把一个方向称为 "右"，另一个称为 "左"。我们现在可以扫描所有其他的约束，对于每一个约束，计算它与这条直线的交点，以及它是 "面向 "右边还是左边（即，该点的哪一边满足约束）。我们找到所有约束条件的最右边的交点，朝向
的最右边的交点和所有面向左边的交点。如果它们交叉，那么就没有解决方案。否则，解决方案是哪一个端点给出了更好的c-x值（如果它们给出了相同的值，即 如果它们给出相同的值--即直线am-x=bm与c垂直--那么就说我们取最右边的点）。这里花费的总时间是O(m)，因为我们有m-1个约束条件需要扫描，处理每个约束条件需要O(1)个时间。
现在，对于d=2来说，这看起来是一个O(m2)时间的算法，因为如果情况2发生，我们有可能花费O(m)时间来添加一个新的约束。但是，假设我们以一个 随机顺序添加？约束条件m进入情况2的概率是多少？
请注意，所有m个约束的最优解（假设LP是可行的和有界的）是在可行区域的一个角落，这个角落是由两个约束定义的，即多边形的两条边在该点相遇。如果这两个约束条件都已经被看到了，那么我们就保证处于情况1中。因此，如果我们以随机的顺序插入约束条件，当我们到达约束条件m时，我们处于情况2的概率最多为2/m。这意味着插入第m个约束条件的预期成本最多为。
E[插入第m个约束的成本] ≤ (1-2/m)O(1) + (2/m)O(m) = O(1)。
这有时被称为 "逆向分析"，因为我们所说的是，如果我们向后看，从我们拥有的m个约束中随机抽出一个约束，它是其中一个约束的机会
的机会最多为2/m。
因此，塞德尔的算法如下。将约束条件按随机顺序放置，每次插入一个，如上所述，保持到目前为止的最佳解决方案的跟踪。我们刚刚表明，第1次插入的预期 如果你愿意的话，我们表明T(m)=O(1)+T(m - 1)，其中T(i)是有i个约束的问题的预期成本），所以总体预期成本是O(m)。
如果LP是不可行的呢？我们可以从两个方面来分析。一是如果LP是不可行的，那么事实证明这是由最多三个约束条件决定的。所以我们得到的结果与上面一样，用3/m代替2/m。另一种分析方法是设想我们有一个单独的账户，我们可以用来支付我们到达案例2并发现LP不可行的事件。
由于这在整个过程中只能发生一次（一旦我们确定LP不可行，我们就停止），这只是提供了一个加法的O(m)项。换句话说，如果系统是不可行的，那么最后的约束条件会有两种情况。(a)在此之前是可行的，在这种情况下，我们从额外的预算中支付O(m)（但上述分析适用于（可行的）前m-1个约束），或者(b)它已经是不可行的，在这种情况下，我们已经停止了，所以我们支付0。
那么，无界性呢？我们可以处理这个问题的一个方法是把所有东西都放在一个边界框内-λ≤xi≤λ（因此，例如，如果所有ci都是正的，那么初始x∗=（λ， ...，λ）），我们把λ象征性地看作一个极限数量。例如，在二维空间中，如果c = (0, 1)，我们有一个像2x1 + x2 ≤ 8这样的约束，那么我们会发现它不被(λ, λ)所满足，将该约束与盒子相交并更新为x∗ = (4 - λ/2, λ)。
到目前为止，我们已经表明，对于d=2，算法的预期运行时间是O(m)。对于一般的d值，有两个主要变化。首先，约束条件m进入的概率
情况2现在是d/m而不是2/m。第二，我们需要计算在情况2中执行更新的时间。然而，请注意，这是一个(d - 1)-维线性规划问题，因此我们可以递归地使用相同的算法，在我们花了O(dm)时间将m - 1个约束条件中的每一个投影到(d - 1)-维超平面am - x = bm之后。把这些放在一起，我们有一个预期运行时间的递归。T(d, m) ≤ T(d, m - 1) + O(d) + dm[O(dm) + T(d - 1, m - 1) ] 。
然后求解为T(d, m) = O(d!m)。







通过www.DeepL.com/Translator（免费版）翻译



# Misra-Gries算法求最频繁项

<img src="C:\Users\JESSICA\AppData\Roaming\Typora\typora-user-images\image-20211117233159208.png" alt="image-20211117233159208" style="zoom:67%;" />



#### 发生减法的轮数

当内存中计数器都用完之后，才会出现计数器减一的操作。此时，计数器值共减少k，包括被舍弃的新数据项，计数器值之和共比实际到达的数据项的个数少k+1。

设n是数据流中所有元素出现的次数，n'是当前所有计数器之和。计数器减1的操作有$\frac{n-n'}{k+1}$次。若最后的计数器值之和是大于等于0的，计数器减一的操作最多执行了$\frac{n}{k+1}$轮。

#### 正确性

数据流中有n个数，每个数$n_i$对应的频次为$f_i$

当 最频繁项的频次<发生减法的轮数 时， 数据可能丢失

最多减少了$\frac{n}{k+1}$轮

当$f_i>\frac{n}{k+1}$时，数据就不会丢失了。

即$k+1>\frac{n}{f_i}=\frac{1}{ϕ}$，是一个准确的算法。





我们首先对**频繁项**进行形式化的定义。

给定一系列数据项，频繁项挖掘的目的只是简单地找到那些出现最频繁的数据项。通常我们定义这个问题为找到那些出现频率超过具体阈值的数据项。

**定义1.** 给定一个数据流S，它包含n个数据项$t_1,⋯,t_n$，那么一个数据项$i$的频数为$f_i$。而集合$f_i>ϕ$中的元素，我们称为$ϕ−频繁项$。

**例子.** 对于数据流S=(a,b,a,c,c,a,b,d),有$f_a=3,f_b=2,f_c=2,f_d=1$。如果设$ϕ=0.2$，那么频繁项有$a,b和c$。($f_a=0.375,f_b=0.25,f_c=0.25,f_d=0.125$)

### Misra-Gries算法

即使ϕ的值很大，解决这个问题的算法也至少要花费O(n)的空间。在这种情况下，一个错误率为ϵ的近似算法被提出。这就是我们的Misra-Gries算法。它的具体步骤如下：

内存最多有k个计数器![image-20211117233159208](C:\Users\JESSICA\AppData\Roaming\Typora\typora-user-images\image-20211117233159208.png)



当完成对数据流的扫描后，内存中保存的$k′(k′≤k)$个元素即是数据流中的频繁项。



上面说到了这个算法是一个近似算法，这表明算法输出的结果并不一定是频繁项。Misra-Gries算法的错误率为ϵ。

**定义2.** 给定一个包含n个数据项的数据流S，上述的ϵ−近似算法返回一个集合F。对于所有满足$i∈F$的数据项i，都有$f_i>(ϕ−ϵ)n$；并且不存在$i∉F$的数据项i，使得$f_i>ϕn$。

上面的定义表明，Misra-Gries算法输出的数据项并不一定是频繁项，但是频繁项一定在输出结果之中。后一句便是问题的关键了，它表明Misra-Gries算法可以确保找到数据流中的频繁项。下面我们对这一点进行简要的证明。

**定理1.** 计数器减一的操作最多执行了$n/k$轮。

证明：当数组T中元素的个数等于k时，才会出现计数器减一的操作。此时，计数器值共减少k，包括被舍弃的新数据项，计数器值之和共比实际到达的数据项的个数少k+1。

设n是数据流中所有元素出现的次数，n'是当前所有计数器之和。计数器减1的操作有$\frac{n-n'}{k+1}$次。

若最后的计数器值之和是大于等于0的，计数器减一的操作最多执行了$\frac{n}{k+1}$轮。

**定理2.** 当$k+1=⌈\frac{1}{ϕ}⌉$，所有的ϕ−频繁项都会被Misra-Gries算法检测出。

证明：由定理1可知，计数器减一的操作最多执行了$\frac{n}{k+1}$轮。因此，算法结束时，数据项i计数器的值$c_i$，满足$c_i≤f_i≤c_i+\frac{n}{k+1}$。对于所有不在数组T中的数据项i，有$c_i=0$，于是$f_i≤\frac{n}{k+1}≤ϕn$。故所有满足$f_j>ϕn$的数据项j，即所有的ϕ−频繁项都会被Misra-Gries算法检测出。







DAG计算

In short, a DAG describes a sequence of execution steps in the complex non-recurring computation



DAG 流



程的数学抽象



输入是不同的文档，每一行是字符串，找不同的worker拿到自己手上，这就是split。然后每个字符串切分

shuffle就是把不同的单词聚合到各自盒子里面，这是shuffle算法决定的。然后reduce就是把相同的数据求和

这个是并行的。



首先有个用户进程，需要定义程序怎么运行，会想要把数据拆成多少份去做，比如用户进程将数据拆成五份，然后会产生很多worker，有map worker和reduce worker，其中最重要会产生一个master worker，master作为用户的代理去协调整个过程，比如这个worker去负责拿0号数据，这个worker去拿1号数据。每个worker会在本地把数据切分，写在本地缓存或硬盘上。这时候master说：reducer，你们去拿数据吧，这时候reducer就会去把自己需要的本地数据拿过来，reduce做完之后将结果写入最终文件里。

